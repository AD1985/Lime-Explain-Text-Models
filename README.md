# Model Interpretability

As the world is gearing towards model driven approach for most decision making, it is important to understand how a machine learning model is reaching to a decision. Most NLP models operate over high number of dimensions and so interpretability becomes even more important in such cases. 

This repository is an effort to explain use of LIME in explaining a simple logistic alrogithm based text classification model. 

Data is taken from Jigsaw Toxic Comment Classification challenge of Kaggle and can be download from [here](https://www.kaggle.com/c/jigsaw-toxic-comment-classification-challenge). This problem was a multi-label problem but I changed it to binary classification problem for ease of explaination of generated results. 

https://nbviewer.jupyter.org/github/AD1985/Lime-Explain-Text-Models/blob/bea859622add13e893d9ca2bd6e2c515bbd3eb0b/Text%20Modelling%20Analyzer.ipynb
